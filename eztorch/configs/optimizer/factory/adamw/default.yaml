_target_: eztorch.optimizers.optimizer_factory
_recursive_: False
exclude_wd_norm: True
exclude_wd_bias: True
name: adamw
params:
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.e-8
  amsgrad: False
batch_size: null
initial_lr: 0.001
layer_decay_lr: null
scaler: null
scheduler: null
