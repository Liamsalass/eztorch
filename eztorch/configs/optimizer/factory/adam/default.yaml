_target_: eztorch.optimizers.optimizer_factory
_recursive_: False
exclude_wd_norm: False
exclude_wd_bias: False
name: adam
params:
  weight_decay: 0.
  betas: [0.9, 0.999]
  eps: 1.e-8
  amsgrad: False
batch_size: null
initial_lr: 0.003
layer_decay_lr: null
scaler: null
scheduler: null
